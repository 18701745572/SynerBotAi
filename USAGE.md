# 📖 使用说明

## 🚀 快速开始

### 1. 启动 LM Studio

1. 打开 LM Studio 应用程序
2. 在模型库中选择并下载您想要使用的大语言模型
3. 加载模型到本地
4. 启动本地服务器：
   - 点击 "Start Server" 按钮
   - 确保服务器运行在端口 1234
   - 确认 API 端点可用

### 2. 启动聊天应用

```bash
# 安装依赖
npm install

# 启动开发服务器
npm run dev
```

应用将在 `http://localhost:3000` 启动

## 🎯 功能使用

### 基本聊天

1. **发送消息**：在输入框中输入您的问题，按回车或点击发送按钮
2. **快捷回复**：点击预设的快捷回复按钮快速发送常见问题
3. **查看回复**：AI 助手的回复会显示在左侧，带有机器人头像

### 配置设置

点击右上角的 "⚙️ 配置设置" 按钮打开配置面板：

#### 🔗 连接设置
- **API 地址**：LM Studio 的 API 地址（默认：http://localhost:1234/v1）
- **超时时间**：请求超时时间（默认：30秒）
- **模型名称**：要使用的模型名称（默认：local-model）

#### 🎛️ 模型参数
- **温度**：控制回复的创造性（0-2，默认：0.7）
  - 0：更确定性，回复更一致
  - 2：更创造性，回复更多样化
- **最大令牌数**：单次回复的最大长度（100-4000，默认：1000）

#### 📝 系统提示词
- 自定义 AI 助手的角色和行为
- 例如：`你是一个专业的编程助手，请用中文回答用户的问题。`

#### 🧪 连接测试
- **测试连接**：检查 LM Studio 服务是否正常运行
- **获取模型列表**：显示所有可用的模型

## 🔧 故障排除

### 连接问题

**问题**：显示 "未连接" 状态
**解决方案**：
1. 确认 LM Studio 正在运行
2. 检查端口 1234 是否被占用
3. 在浏览器中访问 `http://localhost:1234/v1/models` 测试连接
4. 检查防火墙设置

**问题**：API 请求失败
**解决方案**：
1. 确认模型已正确加载
2. 检查模型名称是否正确
3. 增加超时时间设置
4. 查看浏览器控制台的错误信息

### 模型问题

**问题**：模型不存在错误
**解决方案**：
1. 在 LM Studio 中确认模型已加载
2. 使用 "获取模型列表" 功能查看可用模型
3. 更新配置中的模型名称

**问题**：回复质量不佳
**解决方案**：
1. 调整温度参数（降低温度获得更一致的回复）
2. 增加最大令牌数以获得更详细的回复
3. 优化系统提示词以获得更好的角色定义

## 💡 使用技巧

### 优化对话体验

1. **使用清晰的提示词**：具体描述您的需求
2. **提供上下文**：在复杂问题中提供必要的背景信息
3. **分步骤提问**：将复杂问题分解为多个简单问题
4. **使用快捷回复**：快速开始常见类型的对话

### 自定义配置

1. **保存配置**：所有配置会自动保存到本地存储
2. **多模型支持**：可以切换不同的模型进行对比
3. **参数调优**：根据使用场景调整温度和令牌数
4. **角色定制**：通过系统提示词定义 AI 助手的专业领域

### 高级功能

1. **错误处理**：应用会自动处理网络错误和 API 错误
2. **连接监控**：实时显示 LM Studio 连接状态
3. **响应式设计**：支持桌面和移动设备
4. **本地存储**：配置和对话历史会保存在本地

## 🔄 更新和维护

### 检查更新
```bash
# 更新依赖
npm update

# 重新构建
npm run build
```

### 备份配置
配置文件保存在浏览器的本地存储中，可以通过以下方式备份：
1. 打开浏览器开发者工具
2. 进入 Application/Storage 标签
3. 找到 Local Storage 中的 `lmStudioConfig` 项
4. 复制配置值进行备份

## 📞 获取帮助

如果遇到问题：
1. 查看浏览器控制台的错误信息
2. 检查 LM Studio 的日志输出
3. 参考 README.md 中的故障排除部分
4. 确认所有依赖都已正确安装
